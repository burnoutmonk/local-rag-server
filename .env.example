# Copy this file to .env and adjust the values

# ── Model ─────────────────────────────────────────────────────────────────────
# Filename of the GGUF model (will be downloaded automatically if not present)
LLM_MODEL_FILE=qwen2.5-3b-instruct-q4_k_m.gguf

# HuggingFace repo to download from — set to empty to disable auto-download
LLM_MODEL_REPO=Qwen/Qwen2.5-3B-Instruct-GGUF

# ── LLM server ────────────────────────────────────────────────────────────────
LLM_CONTEXT=4096
LLM_THREADS=8
LLM_GPU_LAYERS=0   # set to -1 to enable full GPU offload
