LLM_MODEL_FILE=Llama-3.2-3B-Instruct-Q4_K_M.gguf
LLM_MODEL_REPO=bartowski/Llama-3.2-3B-Instruct-GGUF
LLM_MODEL_PATH=models/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# ── Embedding model ───────────────────────────────────────────────────────────
# Changing this requires a full re-ingest (docker volume rm local_rag_ingest_state)
EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# ── LLM server ────────────────────────────────────────────────────────────────
# CPU: 4096  |  GPU: 32768 (or higher)
LLM_CONTEXT=4096 
LLM_THREADS=8
# GPU layers: set to -1 to offload all layers to GPU, 0 for CPU only
LLM_GPU_LAYERS=0
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.8
LLM_TOP_K=20
LLM_MIN_P=0.0

# ── Output tokens ─────────────────────────────────────────────────────────────
# slow CPU: 200-300 | fast CPU/GPU: 500-900
MAX_TOKENS=500
MIN_TOKENS=150
# run test_speed.py to measure your hardware
TOKENS_PER_SECOND=10.0

# ── Chunking ──────────────────────────────────────────────────────────────────
MAX_CHARS=1000
OVERLAP_CHARS=100

# ── Chat memory ───────────────────────────────────────────────────────────────
# Number of previous exchanges to include as context per browser session (0 to disable)
CHAT_MEMORY_TURNS=3

# ── Hybrid retrieval ──────────────────────────────────────────────────────────
# BM25 fusion weight: 0.0 = dense only, 1.0 = BM25 only, 0.5 = equal blend
BM25_WEIGHT=0.5
# Candidates fetched = top_k * RETRIEVAL_MULTIPLIER (before BM25/rerank filtering)
RETRIEVAL_MULTIPLIER=4

# ── Reranker ──────────────────────────────────────────────────────────────────
# Cross-encoder model for reranking (set RERANKER_ENABLED=false to disable globally)
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANKER_ENABLED=true

# ── GPU build (Docker only) ───────────────────────────────────────────────────
# set to true to build llama.cpp with CUDA support
CUDA_AVAILABLE=true
